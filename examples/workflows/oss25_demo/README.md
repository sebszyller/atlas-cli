# Open Source Summit North America (OSS NA) '25 Provenance Demo

## Introduction
This example demonstrates how to generate and verify provenance data for a
two-stage machine learning lifecycle using the Atlas CLI tool and the Atlas Test Framework. Although the demo
contains scripts for training and evaluation pipelines, the demo does not run
them but still tracks these software components as part of the lifecycle from
dataset download to evaluation, showing how to establish an end-to-end audit
trail of all artifacts and their relationships.

To show the added integrity properties gained from running the Atlas CLI inside
of a confidential computing environment such as Intel TDX, the demo will
collect a hardware-based platform attestation and include it in each artifact
manifest generated by the CLI. When run on platforms that do not support Intel
TDX, the demo will still generate a mock hardware attestation.

This example creates C2PA-compliant manifests for:
- Datasets (raw MNIST training and test datasets downloaded from HuggingFace)
- Software components (training and evaluation scripts)
- Models (dummy trained classifier model)
- Evaluation results (dummy results)

All components are linked to their direct parents during creation to form a
complete provenance graph that can be exported and audited.

For a more comprehensive example that does run the training and evaluation
pipelines, see the
[MNIST training provenance collection example](../mnist_pipeline/README.md).

## Prerequisites

### System Requirements
- Python 3.8 or above with Poetry
- Rust toolchain (1.85 or above)
- Docker and Docker-compose

### Setting up Atlas CLI
Ensure Atlas CLI is built and available in your PATH:
```bash
# Build Atlas CLI (from the root directory)
cargo build --release
# Add to PATH or use full path
export PATH=$PATH:./target/release
```

### Setting up the Atlas Test Framework
See the parent [README.md](../../README.md) for instructions on building the test framework.

### Setting up the Database Backend
Start the database backend (if not already running):
```bash
# Start the database service
cd storage_service && docker-compose build && docker-compose up -d && cd ..
```

## Running the Example

You can launch the demo using the Atlas Test Framework with the provided YAML configuration:

```bash
# Run the OSS NA '25 demo
./target/release/atlas-test examples/oss25_demo/oss25_demo_pipeline.yaml
```

The demo runs in interactive mode by default, allowing you to progress through each step by pressing Enter. The framework will automatically:
- Generate signing keys
- Track all manifest IDs between steps
- Display JSON outputs with proper formatting
- Create a complete audit trail
- Generate a reproduction script

### Alternative Run Options

```bash
# Run without interactive pauses
atlas-test examples/oss25_demo/oss25_demo_pipeline.yaml --no-interactive

# Dry run to see what commands would be executed
./target/release/atlas-test oss25_demo_pipeline.yaml --dry-run

# Verbose output for debugging
./target/release/atlas-test oss25_demo_pipeline.yaml --verbose
```

### Demo Steps

The YAML configuration (`oss25_demo_pipeline.yaml`) orchestrates the following steps:

1. **Download MNIST Datasets**: Downloads training and test datasets from HuggingFace in Parquet format
2. **Generate Training Data Provenance**: Creates and displays manifest for MNIST training dataset
3. **Generate Training Artifacts Provenance**: 
   - Creates manifest for training script with TDX attestation
   - Creates manifest for dummy model file
   - Links training dataset and script to the model
4. **Generate Evaluation Artifacts Provenance**:
   - Creates manifest for test dataset
   - Creates manifest for evaluation script with TDX attestation
   - Creates evaluation results manifest
   - Links evaluation script to results
5. **Export Provenance Graph**: Generates complete provenance graph in JSON format
6. **Validation**: 
   - Validates all manifest integrity
   - Tests invalid manifest link (expected failure)
   - Displays final provenance graph

Each step includes appropriate pauses for demonstration purposes and displays the generated manifests in formatted JSON.

## Output Files

The demo generates several output files in the `./output/` directory:
- `train_dataset_manifest.json` - Training dataset manifest
- `model_manifest.json` - Initial model manifest
- `linked_model_manifest.json` - Model manifest with linked dependencies
- `mnist_provenance.json` - Complete provenance graph
- `commands.log` - Complete command execution log
- `reproduce.sh` - Executable script to reproduce the demo

## Framework Features

The Atlas Test Framework provides several advantages over the original bash script:
- **Automatic ID tracking**: No manual ID extraction needed
- **Error handling**: Built-in error detection and reporting
- **Reproducibility**: Generates scripts to reproduce exact runs
- **Interactive control**: Configurable pauses and interaction
- **Comprehensive logging**: Detailed execution logs
- **Validation**: Built-in manifest validation and verification

## Troubleshooting

If you encounter issues:

1. **Check Atlas CLI**: Ensure `atlas-cli --version` works
2. **Check Database**: Verify the storage service is running on `localhost:8080`
3. **Check Dependencies**: Run `poetry install` to ensure all dependencies are available
4. **Verbose Mode**: Use `--verbose` flag for detailed debugging output
5. **Logs**: Check `./output/commands.log` for detailed command execution history

## Customization

You can modify the demo by editing `oss25_demo_pipeline.yaml`:
- Change dataset sources or file paths
- Modify author information
- Add additional validation steps
- Adjust TDX attestation settings
- Configure different storage backends